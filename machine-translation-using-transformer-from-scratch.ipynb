{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":8311007,"sourceType":"datasetVersion","datasetId":4937140},{"sourceId":1878727,"sourceType":"datasetVersion","datasetId":1118439}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Overview \nIn this notebook, I'll be focusing on machine translation using a Bengali to English dataset. I spent 10 days gathering the data, resulting in a clean dataset containing a total of 4 million rows. It's worth noting that this dataset is not yet public. Nonetheless, I'll attempt to train a Transformer architecture model for machine translation.","metadata":{}},{"cell_type":"markdown","source":"# Import Dependencies","metadata":{}},{"cell_type":"code","source":"import os\nimport sys\nimport math\nimport copy\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\ntqdm.pandas()\nimport json\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.utils.data as data\n\nimport torchtext\nimport datetime\nimport pathlib\nimport io\nimport os\nimport re\nimport string\nimport time\nfrom numpy import random\nimport tensorflow_datasets as tfds\nimport tensorflow_probability as tfp\nfrom keras.models import Model\nfrom keras.layers import Layer\nfrom keras.layers import (Dense,Flatten,SimpleRNN,InputLayer,Conv1D,Bidirectional,GRU,LSTM,BatchNormalization,Dropout,Input, Embedding,TextVectorization)\nfrom keras.losses import BinaryCrossentropy,CategoricalCrossentropy, SparseCategoricalCrossentropy\nfrom keras.metrics import Accuracy,TopKCategoricalAccuracy, CategoricalAccuracy, SparseCategoricalAccuracy\nfrom keras.optimizers import Adam\nfrom keras.layers import MultiHeadAttention, LayerNormalization\nfrom tensorboard.plugins import projector","metadata":{"execution":{"iopub.status.busy":"2024-05-08T10:34:37.081741Z","iopub.execute_input":"2024-05-08T10:34:37.082163Z","iopub.status.idle":"2024-05-08T10:34:58.124717Z","shell.execute_reply.started":"2024-05-08T10:34:37.082134Z","shell.execute_reply":"2024-05-08T10:34:58.123554Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-05-08 10:34:47.965860: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-08 10:34:47.965983: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-08 10:34:48.127584: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/no-more-low-resources-bengali-machine-translation/final_data.csv')","metadata":{"execution":{"iopub.status.busy":"2024-05-08T10:34:58.126785Z","iopub.execute_input":"2024-05-08T10:34:58.128179Z","iopub.status.idle":"2024-05-08T10:35:22.923756Z","shell.execute_reply.started":"2024-05-08T10:34:58.128136Z","shell.execute_reply":"2024-05-08T10:35:22.922730Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"print(df.shape)\nprint(df.isnull().sum())\nprint(df.duplicated().sum())","metadata":{"execution":{"iopub.status.busy":"2024-05-04T16:32:06.442377Z","iopub.execute_input":"2024-05-04T16:32:06.442813Z","iopub.status.idle":"2024-05-04T16:32:13.971833Z","shell.execute_reply.started":"2024-05-04T16:32:06.442782Z","shell.execute_reply":"2024-05-04T16:32:13.970554Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"(3288975, 2)\nbn    10\nen    38\ndtype: int64\n0\n","output_type":"stream"}]},{"cell_type":"code","source":"df.dropna(inplace = True)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T10:36:15.576797Z","iopub.execute_input":"2024-05-08T10:36:15.577192Z","iopub.status.idle":"2024-05-08T10:36:16.584604Z","shell.execute_reply.started":"2024-05-08T10:36:15.577160Z","shell.execute_reply":"2024-05-08T10:36:16.583796Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df = df.head(1000000)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T10:36:18.253556Z","iopub.execute_input":"2024-05-08T10:36:18.253902Z","iopub.status.idle":"2024-05-08T10:36:18.258536Z","shell.execute_reply.started":"2024-05-08T10:36:18.253873Z","shell.execute_reply":"2024-05-08T10:36:18.257575Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"print(df.isnull().sum())\nprint(df.duplicated().sum())","metadata":{"execution":{"iopub.status.busy":"2024-05-08T10:32:23.007996Z","iopub.execute_input":"2024-05-08T10:32:23.008374Z","iopub.status.idle":"2024-05-08T10:32:24.767283Z","shell.execute_reply.started":"2024-05-08T10:32:23.008343Z","shell.execute_reply":"2024-05-08T10:32:24.766221Z"},"trusted":true},"execution_count":49,"outputs":[{"name":"stdout","text":"bn    0\nen    0\ndtype: int64\n0\n","output_type":"stream"}]},{"cell_type":"code","source":"for i in range(15):\n    print(df['bn'][i+1])\n    print(df['en'][i+1])","metadata":{"execution":{"iopub.status.busy":"2024-05-04T12:13:07.459690Z","iopub.execute_input":"2024-05-04T12:13:07.459992Z","iopub.status.idle":"2024-05-04T12:13:07.477917Z","shell.execute_reply.started":"2024-05-04T12:13:07.459965Z","shell.execute_reply":"2024-05-04T12:13:07.476301Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"তুরুপ\ntrump\nপদদলিত করা\noverrides\nসেগুলো হৃদয়কে উষ্ণ করে এবং রোজকার বোঝাগুলোকে হালকা করে\nthey warm the heart and ease the daily load\nআমি ভালোবাসি তোমাকে\ni love you\nপোর্ট কোম্পানি লিমিটেড কেপিসিএল\nport company limited kpcl\nতোলপাড়\ncommotions\nএছাড়াও ক্লেমঁসো উইলসনের ১৪ দফার ব্যাপারে সংশয়ী এবং হতাশ ছিলেন তিনি অভিযোগ করে বলেন মিস্টার উইলসনের ১৪ দফা বিরক্তিকর\nclemenceau also expressed skepticism and frustration with wilsons fourteen points mr wilson bores me with his fourteen points complained clemenceau\nআলজাজিরার সঙ্গে জালুদের সাক্ষাতকার এবং তার দৃষ্টিভঙ্গীর বিষয়ে অন্যান্য টুইটার ব্যবহারকারীরা মন্তব্য করেন\nother twitter users went on commenting on jallouds interview for aljazeera and his attitude\nক্লাব\nclub\nতাই আসুন এখন আমরা একবার পরীক্ষা করে দেখি যে তাদের এই দাবি সত্যি কি না\nso let us investigate on trial\nওমানের শাসক সুলতান কাবুস বিন সাইদের সমালোচনা করার অপরাধে আলরাওয়াহিকে শাস্তি প্রদানের পর গ্লোবাল ভয়েসেস এডভোকেসিতে ২০১২ সালে তাকে লক্ষ্যনীয়ভাবে উপস্থাপন করা হয়\nin 2012 alrawahi was featured on global voices advocacy after he was detained for criticizing sultan qaboos bin said the ruler of oman\nআমাকে না বললেও আমি কাউকে বলতাম না\ni would not tell anybody even if he hadnt said that\nভালুককে একটা আলিঙ্গন করে জড়িয়ে ধরে\nto give the bear a great big hug\n১ পিতর ৫৫ আমরা আমাদের ভাইদের সঙ্গে যেভাবে ব্যবহার করি তা যিহোবার সঙ্গে আমাদের সম্পর্কের ওপর একটা বড় ছাপ ফেলে ১ যোহন ৪২০\n1 peter 55 the way we treat fellow worshipers has a direct bearing on our relationship with god 1 john 420\nদায়িত্ব\nresponsibilities\n","output_type":"stream"}]},{"cell_type":"code","source":"train_ratio = 0.8\nval_ratio = 0.1\ntest_ratio = 0.1\n\nnum_sentences = len(df)\nnum_train = int(train_ratio * num_sentences)\nnum_val = int(val_ratio * num_sentences)\nnum_test = num_sentences - num_train - num_val\n\n# Shuffle the dataset\ndf = df.sample(frac=1).reset_index(drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T10:36:21.918823Z","iopub.execute_input":"2024-05-08T10:36:21.919641Z","iopub.status.idle":"2024-05-08T10:36:22.473188Z","shell.execute_reply.started":"2024-05-08T10:36:21.919599Z","shell.execute_reply":"2024-05-08T10:36:22.472083Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_df = df[:num_train]\nval_df = df[num_train:num_train+num_val]\ntest_df = df[num_train+num_val:]","metadata":{"execution":{"iopub.status.busy":"2024-05-08T10:36:22.544686Z","iopub.execute_input":"2024-05-08T10:36:22.545224Z","iopub.status.idle":"2024-05-08T10:36:22.550053Z","shell.execute_reply.started":"2024-05-08T10:36:22.545196Z","shell.execute_reply":"2024-05-08T10:36:22.549032Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Tokenization","metadata":{}},{"cell_type":"code","source":"train_en_tokens = []\ntrain_bn_tokens = []\nval_en_tokens = []\nval_bn_tokens = []\ntest_en_tokens = []\ntest_bn_tokens = []\nen_vocab = {'<pad>': 0}  # Initialize English vocabulary with <pad> token\nbn_vocab = {'<pad>': 0}  # Initialize Bengali vocabulary with <pad> token\n\ndef tokenize_sentence(sentence, vocab):\n    tokens = sentence.split()\n    token_ids = []\n    for token in tokens:\n        if token not in vocab:\n            vocab[token] = len(vocab)\n        token_ids.append(vocab[token])\n    return token_ids\n\n# Tokenizing training data\nprint(\"Tokenizing training data:\")\nfor en_sent, bn_sent in tqdm(zip(train_df['en'], train_df['bn']), total=len(train_df)):\n    train_en_tokens.append(tokenize_sentence(en_sent, en_vocab))\n    train_bn_tokens.append(tokenize_sentence(bn_sent, bn_vocab))\n\n# Tokenizing validation data\nprint(\"Tokenizing validation data:\")\nfor en_sent, bn_sent in tqdm(zip(val_df['en'], val_df['bn']), total=len(val_df)):\n    val_en_tokens.append(tokenize_sentence(en_sent, en_vocab))\n    val_bn_tokens.append(tokenize_sentence(bn_sent, bn_vocab))\n\n# Tokenizing testing data\nprint(\"Tokenizing testing data:\")\nfor en_sent, bn_sent in tqdm(zip(test_df['en'], test_df['bn']), total=len(test_df)):\n    test_en_tokens.append(tokenize_sentence(en_sent, en_vocab))\n    test_bn_tokens.append(tokenize_sentence(bn_sent, bn_vocab))\n\n# Update the vocabulary sizes\nsrc_vocab_size = len(en_vocab)\ntgt_vocab_size = len(bn_vocab)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-08T10:36:23.557251Z","iopub.execute_input":"2024-05-08T10:36:23.558017Z","iopub.status.idle":"2024-05-08T10:36:41.034631Z","shell.execute_reply.started":"2024-05-08T10:36:23.557982Z","shell.execute_reply":"2024-05-08T10:36:41.033644Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Tokenizing training data:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/800000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a57a44f9f70453fa094ef6a097cea9f"}},"metadata":{}},{"name":"stdout","text":"Tokenizing validation data:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/100000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7dc1f2bf6f646f7b279ea9091e959ed"}},"metadata":{}},{"name":"stdout","text":"Tokenizing testing data:\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/100000 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4d6e5a16f9b42108856da85515caa0b"}},"metadata":{}}]},{"cell_type":"markdown","source":"# Custome Dataset","metadata":{}},{"cell_type":"code","source":"class TranslationDataset(data.Dataset):\n    def __init__(self, en_tokens, bn_tokens):\n        self.en_tokens = en_tokens\n        self.bn_tokens = bn_tokens\n        self.max_len = max(max(len(en), len(bn)) for en, bn in zip(en_tokens, bn_tokens))\n        \n    def __len__(self):\n        return len(self.en_tokens)\n    \n    def __getitem__(self, index):\n        en_data = self.en_tokens[index] + [0] * (self.max_len - len(self.en_tokens[index]))  # Padding with 0\n        bn_data = self.bn_tokens[index] + [0] * (self.max_len - len(self.bn_tokens[index]))  # Padding with 0\n        return torch.tensor(en_data), torch.tensor(bn_data)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T10:42:46.249257Z","iopub.execute_input":"2024-05-08T10:42:46.249890Z","iopub.status.idle":"2024-05-08T10:42:46.257084Z","shell.execute_reply.started":"2024-05-08T10:42:46.249861Z","shell.execute_reply":"2024-05-08T10:42:46.256122Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"train_dataset = TranslationDataset(train_en_tokens, train_bn_tokens)\nval_dataset = TranslationDataset(val_en_tokens, val_bn_tokens)\ntest_dataset = TranslationDataset(test_en_tokens, test_bn_tokens)\n\n# Create data loaders\ntrain_loader = data.DataLoader(train_dataset, batch_size=2, shuffle=True)\nval_loader = data.DataLoader(val_dataset, batch_size=2)\ntest_loader = data.DataLoader(test_dataset, batch_size=2)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-08T10:42:49.128920Z","iopub.execute_input":"2024-05-08T10:42:49.129296Z","iopub.status.idle":"2024-05-08T10:42:49.559107Z","shell.execute_reply.started":"2024-05-08T10:42:49.129269Z","shell.execute_reply":"2024-05-08T10:42:49.558300Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Print the first batch of data from train_loader\nfor batch_idx, (en_data, bn_data) in enumerate(train_loader):\n    print(\"English Data (Batch 0):\", en_data)\n    print(\"Bengali Data (Batch 0):\", bn_data)\n    break  # Break after printing the first batch to avoid printing the entire dataset\n","metadata":{"execution":{"iopub.status.busy":"2024-05-08T10:42:51.660080Z","iopub.execute_input":"2024-05-08T10:42:51.660445Z","iopub.status.idle":"2024-05-08T10:42:51.750803Z","shell.execute_reply.started":"2024-05-08T10:42:51.660416Z","shell.execute_reply":"2024-05-08T10:42:51.749694Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"English Data (Batch 0): tensor([[   32,   201,    42,  ...,     0,     0,     0],\n        [   11, 12390,    42,  ...,     0,     0,     0]])\nBengali Data (Batch 0): tensor([[   31,   136,   374,  ...,     0,     0,     0],\n        [  138, 29632,  6925,  ...,     0,     0,     0]])\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Modeling\n\n<hr>\n<h4>Model Architecture</h4>\n<hr>\n<img src='https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png'>","metadata":{}},{"cell_type":"markdown","source":"# *Step Wise Explanation:*\n* **input Embedding**: The process begins with encoding the input language(e.g English sequenc) into numerical vectors. Each word or token is transformed into a high-dimentional vector. \n\n* **Multi-Head Self-Attention:** This is the heart of the transformer. The model looks at each word in the input sentences and assings different lavels of inportance to other words in the sentence. Multiple attention heads allow the model to focus on different aspects of the sentence simultaneously.\n\n* **Positional Encoding:**  Since transformers don't have inherent sence of word order, positional encoding is added to the word embeddings to help the model understand the words's position in the sentence.\n\n* **Encoder - Decoder Architecture** : In translation task, there are typically two parts: the encoder and the decoder. The encoder takes the input sentence and process it, while the decoder generates the translated output.\n\n* **Decoder Self-Attention** : The decoder also uses multi-head self-attention, but slightly modified to prevent if from looking ahead in the output sentence, which would result in incorrect translations.\n\n* **Attention Output** : The outputs from the attention mechanisms are uesd to calculte attention scores, which datermine how much each word in the input sentence contributes to each word in the output sentence. Position-wise Feedforward Networks: After attention, the model passes the data through feedforward neural networks ot further process and refine the information. Output Layer: The final layer in the decoder procduces probablilities for each word in the targer language vaocabulary, allowing the model to predict the next word in the tanslationl.\n\n* **Training And Optimization** : Transformers are trained using large parralel corpora of source and targer language sentences. They learn to minimize the difference between predicted tranlations and the actual translations in the training data.\n\n* **Repeat For Each Token** : This process is repeated for each word in the output sentence, where the previously generated words are used as context for generating the next word. Beam Search or Greedy Decoding: During inference, the model generates translations one word at a time. Beam search or greedy decoding is often used to select the most likely next word based on the model's predictions.","metadata":{}},{"cell_type":"markdown","source":"<h4>Inside Attention Layer</h4>\n<img src='https://production-media.paperswithcode.com/methods/35184258-10f5-4cd0-8de3-bd9bc8f88dc3.png'>","metadata":{}},{"cell_type":"markdown","source":"# Easy to understand Explanation:\n\nLet's break down and realte it to the components and process in a transformer model:\n\n   *  School and Studens: Think of the school as the entire context, and the students as the individual tokens in a sequence.\n\n   *  Vecitorization and Tokenization : the process of converting students into tokens and vectorizing them represents in the initial preprocessing steps where text data is tokenized into individual words or tokens and then converted into numerical vector representations.\n\n   * Vocabulary: The vocabulary of the school represents the set of unique tokens (students) that the model has learned from previous schools within the same company. Thse tokens are sued to represent words in the sequence.\n\n   * Intrs-Attention (Self-Attention) : Each student's interaction with their classmates represents the  intra-attention mechanishm, where relationships, influences, and context between tokenss (students) are captured. Each studen becomes a query (Q), and their classmates become keys (K) and values (V). Attention scores are calculated to determine how much weight each student should give to their calssmates. Softmax normalization of attention scores can be thought of as grading each student's relationships and influence each others. Concatenation of information from different teachers (heads) captures diverse insights.\n\n   * Linear Layer: The linear layer represents the post-attention but post-attention precessing step that helps combine and refine information before producing the final output.\n\n\nThis is the essence of how attention mechanishm work in t transformers, where tokens (students) atten to  each other, calculate their influence, and produce context vectors (mark sheets) for each other. These context vocetors are then in cross-attention to compare tokens from different parts of the model, ultimately leading to the model's final output\n\nEncoder's Role(intra-attention in encoder): the encoder preocess the input sequence and performs intra-attention. it produces context vectors (contextual representation) for each word in the input sequence. Thesee context vecotrs capture information about how each word related to others within input sequence\n\nSignaling the Decoder: The decoder is signaled to start generating the output sequence. Typically , this is fone by providing the decoder with an intial input, often a special start token (e.g ,or).\n\nGenerating the first Word: For the first word in the output sequence, the decoder combines the follwing: The start token as the initail query. The encoder's context vectors, which represent the input sequence. The decoder's own context vector for the output sequence (initialzed explicitly). These componentes are used to predicte the word in the output sequence.\n\nSubssequent Word Preditions: For genrating subsequent words in the output sequence, the following process occurs: The shifted target (previously generated word) becomes the query. The encodr's enctext vector, representing the input sequence, are used for context. The context vectors for the target word (which includes context from the encoder) are also considered. The last word's hidden state, obtained from the decoder's self-attention (intra-attention), is incorporated. These components collectively contribute to the prediction of each subsequent word in the output sequence.\n\nIerative toekn Generation: The decoder repeats the process of generating tokens one by one, considering context from both the encoder's input sequence and it's own generated sequence. At each step, the decoder calculates a probability distribution over the vocabulary for the next token and selects the token with the highest probability.\n\nEnding the sequence: The process continues until the model generates an end token or  reaches a predefined maximum sequence length","metadata":{}},{"cell_type":"markdown","source":" # Transformer Architecture\n \n <img src=\"https://www.mihaileric.com/static/feedforward_layer_and_normalization-dfdcfbd00009f7f99eca73ae29f2dfb7-4ec3a.png\">","metadata":{}},{"cell_type":"markdown","source":"# Positional Encoding","metadata":{}},{"cell_type":"code","source":"class PositionWiseFeedForward(nn.Module):\n    def __init__(self, d_model, d_ff):\n        super(PositionWiseFeedForward, self).__init__()\n        self.fc1 = nn.Linear(d_model, d_ff)\n        self.fc2 = nn.Linear(d_ff, d_model)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        return self.fc2(self.relu(self.fc1(x)))","metadata":{"execution":{"iopub.status.busy":"2024-05-08T10:37:18.176477Z","iopub.execute_input":"2024-05-08T10:37:18.177296Z","iopub.status.idle":"2024-05-08T10:37:18.182979Z","shell.execute_reply.started":"2024-05-08T10:37:18.177265Z","shell.execute_reply":"2024-05-08T10:37:18.182007Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PositionalEncoding(nn.Module):\n    def __init__(self, d_model, max_seq_length):\n        super(PositionalEncoding, self).__init__()\n        \n        pe = torch.zeros(max_seq_length, d_model)\n        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n        \n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        \n        self.register_buffer('pe', pe.unsqueeze(0))\n        \n    def forward(self, x):\n        return x + self.pe[:, :x.size(1)]","metadata":{"execution":{"iopub.status.busy":"2024-05-08T10:37:18.865309Z","iopub.execute_input":"2024-05-08T10:37:18.865660Z","iopub.status.idle":"2024-05-08T10:37:18.873895Z","shell.execute_reply.started":"2024-05-08T10:37:18.865630Z","shell.execute_reply":"2024-05-08T10:37:18.872643Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Custome Attention Layer","metadata":{}},{"cell_type":"markdown","source":"\n**Multi-Head-Attention Layer**\n<hr>","metadata":{}},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, d_model, num_heads, debug_str = None):\n        super(MultiHeadAttention, self).__init__()\n        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n        \n        self.d_model = d_model\n        self.num_heads = num_heads\n        self.d_k = d_model // num_heads\n        \n        self.W_q = nn.Linear(d_model, d_model)\n        self.W_k = nn.Linear(d_model, d_model)\n        self.W_v = nn.Linear(d_model, d_model)\n        self.W_o = nn.Linear(d_model, d_model)\n        self.debug_str = debug_str\n        \n    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n        if (self.debug_str == 'cross'):\n            print('attn_scores:',attn_scores.shape, mask.shape)\n        if mask is not None:\n            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n        attn_probs = torch.softmax(attn_scores, dim=-1)\n        output = torch.matmul(attn_probs, V)\n        return output\n        \n    def split_heads(self, x):\n        batch_size, seq_length, d_model = x.size()\n        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2)\n        \n    def combine_heads(self, x):\n        batch_size, _, seq_length, d_k = x.size()\n        return x.transpose(1, 2).contiguous().view(batch_size, seq_length, self.d_model)\n        \n    def forward(self, Q, K, V, mask=None):\n        Q = self.split_heads(self.W_q(Q))\n        K = self.split_heads(self.W_k(K))\n        V = self.split_heads(self.W_v(V))\n        \n        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n        output = self.W_o(self.combine_heads(attn_output))\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-05-08T10:37:19.667587Z","iopub.execute_input":"2024-05-08T10:37:19.668662Z","iopub.status.idle":"2024-05-08T10:37:19.687582Z","shell.execute_reply.started":"2024-05-08T10:37:19.668611Z","shell.execute_reply":"2024-05-08T10:37:19.685968Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Encoder Layer","metadata":{}},{"cell_type":"code","source":"class EncoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout):\n        super(EncoderLayer, self).__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x, mask):\n        attn_output = self.self_attn(x, x, x, mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        ff_output = self.feed_forward(x)\n        x = self.norm2(x + self.dropout(ff_output))\n        return x","metadata":{"execution":{"iopub.status.busy":"2024-05-08T10:37:20.126578Z","iopub.execute_input":"2024-05-08T10:37:20.126932Z","iopub.status.idle":"2024-05-08T10:37:20.134561Z","shell.execute_reply.started":"2024-05-08T10:37:20.126902Z","shell.execute_reply":"2024-05-08T10:37:20.133508Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Decoder","metadata":{}},{"cell_type":"code","source":"class DecoderLayer(nn.Module):\n    def __init__(self, d_model, num_heads, d_ff, dropout):\n        super(DecoderLayer, self).__init__()\n        self.self_attn = MultiHeadAttention(d_model, num_heads)\n        self.cross_attn = MultiHeadAttention(d_model, num_heads)#, debug_str=\"cross\")\n        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n        self.norm1 = nn.LayerNorm(d_model)\n        self.norm2 = nn.LayerNorm(d_model)\n        self.norm3 = nn.LayerNorm(d_model)\n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, x, enc_output, src_mask, tgt_mask):\n        attn_output = self.self_attn(x, x, x, tgt_mask)\n        x = self.norm1(x + self.dropout(attn_output))\n        \n        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n        x = self.norm2(x + self.dropout(attn_output))\n        \n        ff_output = self.feed_forward(x)\n        x = self.norm3(x + self.dropout(ff_output))\n        return x\n","metadata":{"execution":{"iopub.status.busy":"2024-05-08T10:37:20.708534Z","iopub.execute_input":"2024-05-08T10:37:20.709366Z","iopub.status.idle":"2024-05-08T10:37:20.720090Z","shell.execute_reply.started":"2024-05-08T10:37:20.709323Z","shell.execute_reply":"2024-05-08T10:37:20.718881Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"\nAs you see the combined mask does not consider the pad tokens.\nLets define each of the mask roles:\n- Padding mask - removes consideration of unnecessary pad tokens.\n- Causal mask - Prevents from peeking in the future and helps decoder in predicting one token at a time.\n- Cross-Attention mask - In the context of cross-attention between the encoder and decoder, a mask is used to ensure that the decoder only attends to positions in the encoder that have valid information. In this case, it can be similar to a padding mask when dealing with sequences of different lengths.\n- Combined mask - takes the best of both world and  It ensures that the decoder doesn't include padding tokens in its consideration (like the padding mask) and enforces the autoregressive behavior (like the causal mask), allowing the decoder to predict one token at a time while avoiding future tokens.\n","metadata":{}},{"cell_type":"markdown","source":"# Full Transformer Model","metadata":{}},{"cell_type":"code","source":"\nclass Transformer(nn.Module):\n    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout, pad_token_src = 0, pad_token_tgt = 0, device = 'cpu'):\n        super(Transformer, self).__init__()\n        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n\n        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n\n        self.fc = nn.Linear(d_model, tgt_vocab_size)\n        self.dropout = nn.Dropout(dropout)\n        \n        self.pad_token_src = pad_token_src\n        self.pad_token_tgt = pad_token_tgt\n        self.device = device\n        self = self.to(self.device)\n\n    def generate_mask(self, src_mask, tgt_mask):\n        src_mask = src_mask.unsqueeze(1).unsqueeze(2)\n        tgt_mask = tgt_mask.unsqueeze(1).unsqueeze(3)\n        seq_length = tgt_mask.size(2)\n        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n        tgt_mask = tgt_mask & nopeak_mask.to(self.device)\n        return src_mask, tgt_mask\n\n    def decode(self, src, bos_token_id, eos_token_id, mask=None, max_dec_length = 25):\n        \"\"\"\n        for inference\n        Args:\n            src: input to encoder \n            trg: input to decoder\n        out:\n            out_labels : returns final prediction of sequence\n        \"\"\"\n\n        tgt = torch.tensor([[bos_token_id]]*src.shape[0]).to(self.device)\n        if mask:\n            src_mask, tgt_mask = self.generate_mask(mask['src_mask'], mask['tgt_mask'])\n        else:\n            src_mask, tgt_mask = self.generate_mask(src!=self.pad_token_src, tgt!=self.pad_token_tgt)\n        \n        enc_output = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n        for enc_layer in self.encoder_layers:\n            enc_output = enc_layer(enc_output, src_mask)\n\n        out_labels = tgt\n        unfinished_seq = np.array([1]*src.shape[0])\n        i=0;\n        while (sum(unfinished_seq)>0 & i<max_dec_length):\n            dec_output = self.dropout(self.positional_encoding(self.decoder_embedding(out_labels)))\n            for dec_layer in self.decoder_layers:\n                dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n            output = self.fc(dec_output)\n\n            out_labels = torch.cat((out_labels, output[:,-1:,:].argmax(-1)),dim=1)\n            \n            unfinished_seq[(out_labels[:,-1] == eos_token_id).cpu().numpy()] = 0\n\n            i += 1;\n        return out_labels\n    \n    def forward(self, src, tgt, mask = None):\n        if mask:\n            src_mask, tgt_mask = self.generate_mask(mask['src_mask'], mask['tgt_mask'])\n        else:\n            src_mask, tgt_mask = self.generate_mask(src!=self.pad_token_src, tgt!=self.pad_token_tgt)\n                \n        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n\n        enc_output = src_embedded\n        for enc_layer in self.encoder_layers:\n            enc_output = enc_layer(enc_output, src_mask)\n\n        dec_output = tgt_embedded\n        for dec_layer in self.decoder_layers:\n            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n\n        output = self.fc(dec_output)\n        return output","metadata":{"execution":{"iopub.status.busy":"2024-05-08T10:37:21.662281Z","iopub.execute_input":"2024-05-08T10:37:21.663054Z","iopub.status.idle":"2024-05-08T10:37:21.684980Z","shell.execute_reply.started":"2024-05-08T10:37:21.663014Z","shell.execute_reply":"2024-05-08T10:37:21.683932Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\n# Hyperparameters\nsrc_vocab_size = len(en_vocab)\ntgt_vocab_size = len(bn_vocab)\n# Reduce Model Size\nd_model = 64  # Decrease the model dimensionality\nnum_heads = 2  # Decrease the number of attention heads\nnum_layers = 2  # Decrease the number of layers\nd_ff = 512  # Decrease the size of the feed-forward layers\nmax_seq_length = max(train_dataset.max_len, val_dataset.max_len, test_dataset.max_len)  # Maximum sequence length\ndropout = 0.1  # Dropout probability\n\n# Instantiate the Transformer model\ntransformer_model = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout, device = device)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T10:37:22.327319Z","iopub.execute_input":"2024-05-08T10:37:22.328014Z","iopub.status.idle":"2024-05-08T10:37:23.509315Z","shell.execute_reply.started":"2024-05-08T10:37:22.327982Z","shell.execute_reply":"2024-05-08T10:37:23.508219Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"print(transformer_model)","metadata":{"execution":{"iopub.status.busy":"2024-05-08T10:33:21.883784Z","iopub.execute_input":"2024-05-08T10:33:21.884469Z","iopub.status.idle":"2024-05-08T10:33:21.890222Z","shell.execute_reply.started":"2024-05-08T10:33:21.884434Z","shell.execute_reply":"2024-05-08T10:33:21.889165Z"},"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"Transformer(\n  (encoder_embedding): Embedding(272575, 64)\n  (decoder_embedding): Embedding(466513, 64)\n  (positional_encoding): PositionalEncoding()\n  (encoder_layers): ModuleList(\n    (0-1): 2 x EncoderLayer(\n      (self_attn): MultiHeadAttention(\n        (W_q): Linear(in_features=64, out_features=64, bias=True)\n        (W_k): Linear(in_features=64, out_features=64, bias=True)\n        (W_v): Linear(in_features=64, out_features=64, bias=True)\n        (W_o): Linear(in_features=64, out_features=64, bias=True)\n      )\n      (feed_forward): PositionWiseFeedForward(\n        (fc1): Linear(in_features=64, out_features=512, bias=True)\n        (fc2): Linear(in_features=512, out_features=64, bias=True)\n        (relu): ReLU()\n      )\n      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n  )\n  (decoder_layers): ModuleList(\n    (0-1): 2 x DecoderLayer(\n      (self_attn): MultiHeadAttention(\n        (W_q): Linear(in_features=64, out_features=64, bias=True)\n        (W_k): Linear(in_features=64, out_features=64, bias=True)\n        (W_v): Linear(in_features=64, out_features=64, bias=True)\n        (W_o): Linear(in_features=64, out_features=64, bias=True)\n      )\n      (cross_attn): MultiHeadAttention(\n        (W_q): Linear(in_features=64, out_features=64, bias=True)\n        (W_k): Linear(in_features=64, out_features=64, bias=True)\n        (W_v): Linear(in_features=64, out_features=64, bias=True)\n        (W_o): Linear(in_features=64, out_features=64, bias=True)\n      )\n      (feed_forward): PositionWiseFeedForward(\n        (fc1): Linear(in_features=64, out_features=512, bias=True)\n        (fc2): Linear(in_features=512, out_features=64, bias=True)\n        (relu): ReLU()\n      )\n      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n      (norm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n  )\n  (fc): Linear(in_features=64, out_features=466513, bias=True)\n  (dropout): Dropout(p=0.1, inplace=False)\n)\n","output_type":"stream"}]},{"cell_type":"code","source":"from tqdm import tqdm\n\n# Training loop\nfor epoch in range(num_epochs):\n    transformer_model.train()  # Set the model to training mode\n    total_loss = 0\n    \n    # Create a progress bar\n    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n    \n    # Iterate through batches\n    for batch_idx, (src, tgt) in progress_bar:\n        src, tgt = src.to(device), tgt.to(device)\n        \n        # Zero the gradients\n        optimizer.zero_grad()\n        \n        # Forward pass\n        output = transformer_model(src, tgt[:, :-1])  # Exclude the <eos> token from input\n        \n        # Flatten the output and target tensors to compute loss\n        output_flat = output.view(-1, output.size(-1))\n        tgt_flat = tgt[:, 1:].contiguous().view(-1)  # Exclude the <bos> token from target\n        \n        # Calculate loss\n        loss = criterion(output_flat, tgt_flat)\n        \n        # Backward pass\n        loss.backward()\n        \n        # Clip gradients to prevent exploding gradients\n        torch.nn.utils.clip_grad_norm_(transformer_model.parameters(), max_norm=1)\n        \n        # Update parameters\n        optimizer.step()\n        \n        # Add batch loss to total loss\n        total_loss += loss.item()\n        \n        # Update progress bar description\n        progress_bar.set_postfix({\"Loss\": loss.item()})\n    \n    # Calculate average loss for the epoch\n    avg_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n    \n    # Validation\n    transformer_model.eval()  # Set the model to evaluation mode\n    val_loss = 0\n    \n    with torch.no_grad():\n        # Create a progress bar for validation\n        val_progress_bar = tqdm(enumerate(val_loader), total=len(val_loader), desc=\"Validation\", unit=\"batch\")\n        \n        for batch_idx, (src, tgt) in val_progress_bar:\n            src, tgt = src.to(device), tgt.to(device)\n            \n            # Forward pass\n            output = transformer_model(src, tgt[:, :-1])  # Exclude the <eos> token from input\n            \n            # Flatten the output and target tensors to compute loss\n            output_flat = output.view(-1, output.size(-1))\n            tgt_flat = tgt[:, 1:].contiguous().view(-1)  # Exclude the <bos> token from target\n            \n            # Calculate loss\n            loss = criterion(output_flat, tgt_flat)\n            \n            # Add batch loss to total loss\n            val_loss += loss.item()\n            \n            # Update progress bar description\n            val_progress_bar.set_postfix({\"Validation Loss\": loss.item()})\n    \n    # Calculate average validation lfrom tqdm import tqdm\n\n# Training loop\nfor epoch in range(num_epochs):\n    transformer_model.train()  # Set the model to training mode\n    total_loss = 0\n    \n    # Create a progress bar\n    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\")\n    \n    # Iterate through batches\n    for batch_idx, (src, tgt) in progress_bar:\n        src, tgt = src.to(device), tgt.to(device)\n        \n        # Zero the gradients\n        optimizer.zero_grad()\n        \n        # Forward pass\n        output = transformer_model(src, tgt[:, :-1])  # Exclude the <eos> token from input\n        \n        # Flatten the output and target tensors to compute loss\n        output_flat = output.view(-1, output.size(-1))\n        tgt_flat = tgt[:, 1:].contiguous().view(-1)  # Exclude the <bos> token from target\n        \n        # Calculate loss\n        loss = criterion(output_flat, tgt_flat)\n        \n        # Backward pass\n        loss.backward()\n        \n        # Clip gradients to prevent exploding gradients\n        torch.nn.utils.clip_grad_norm_(transformer_model.parameters(), max_norm=1)\n        \n        # Update parameters\n        optimizer.step()\n        \n        # Add batch loss to total loss\n        total_loss += loss.item()\n        \n        # Update progress bar description\n        progress_bar.set_postfix({\"Loss\": loss.item()})\n    \n    # Calculate average loss for the epoch\n    avg_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n    \n    # Validation\n    transformer_model.eval()  # Set the model to evaluation mode\n    val_loss = 0\n    \n    with torch.no_grad():\n        # Create a progress bar for validation\n        val_progress_bar = tqdm(enumerate(val_loader), total=len(val_loader), desc=\"Validation\", unit=\"batch\")\n        \n        for batch_idx, (src, tgt) in val_progress_bar:\n            src, tgt = src.to(device), tgt.to(device)\n            \n            # Forward pass\n            output = transformer_model(src, tgt[:, :-1])  # Exclude the <eos> token from input\n            \n            # Flatten the output and target tensors to compute loss\n            output_flat = output.view(-1, output.size(-1))\n            tgt_flat = tgt[:, 1:].contiguous().view(-1)  # Exclude the <bos> token from target\n            \n            # Calculate loss\n            loss = criterion(output_flat, tgt_flat)\n            \n            # Add batch loss to total loss\n            val_loss += loss.item()\n            \n            # Update progress bar description\n            val_progress_bar.set_postfix({\"Validation Loss\": loss.item()})\n    \n    # Calculate average validation loss\n    avg_val_loss = val_loss / len(val_loader)\n    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n\n# Save the trained model\ntorch.save(transformer_model.state_dict(), 'transformer_model.pth')\noss\n    avg_val_loss = val_loss / len(val_loader)\n    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n\n# Save the trained model\ntorch.save(transformer_model.state_dict(), 'transformer_model.pth')\n","metadata":{"execution":{"iopub.status.busy":"2024-05-08T10:43:34.356882Z","iopub.execute_input":"2024-05-08T10:43:34.357345Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Epoch 1/10:   4%|▎         | 14210/400000 [42:32<19:09:54,  5.59batch/s, Loss=9.21]","output_type":"stream"}]},{"cell_type":"code","source":"max_seq_length","metadata":{"execution":{"iopub.status.busy":"2024-05-08T10:21:50.842937Z","iopub.execute_input":"2024-05-08T10:21:50.843366Z","iopub.status.idle":"2024-05-08T10:21:50.850207Z","shell.execute_reply.started":"2024-05-08T10:21:50.843333Z","shell.execute_reply":"2024-05-08T10:21:50.848809Z"},"trusted":true},"execution_count":43,"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"1204"},"metadata":{}}]},{"cell_type":"markdown","source":"# save the model","metadata":{}},{"cell_type":"code","source":"PATH = \"./transformer_overfit.pth\"\n# PATH = f\"./transformer_epoch_{epoch}_batch_{batch}.pth\"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torchtext.data.utils import get_tokenizer\n\ndef translate_english_to_bengali(model, src_text, en_vocab, bn_vocab, device='cpu', max_length=50):\n    # Tokenize the input English text\n    tokenizer = get_tokenizer(\"basic_english\")\n    src_tokens = tokenizer(src_text)\n    \n    # Convert tokens to indices using the English vocabulary\n    src_indices = [en_vocab[token] for token in src_tokens]\n    \n    # Convert indices to tensor and add batch dimension\n    src_tensor = torch.tensor(src_indices, dtype=torch.long, device=device).unsqueeze(0)\n    \n    # Generate mask for the source input\n    src_mask = (src_tensor != model.pad_token_src).to(device)\n    \n    # Translate the English text to Bengali\n    with torch.no_grad():\n        # Generate the translation\n        translation_tensor = model.decode(src_tensor, en_vocab['<bos>'], en_vocab['<eos>'], mask={'src_mask': src_mask})\n    \n    # Convert translation tensor to list of indices\n    translation_indices = translation_tensor.squeeze(0).cpu().tolist()\n    \n    # Convert indices to Bengali tokens\n    translation_tokens = [bn_vocab.itos[idx] for idx in translation_indices]\n    \n    # Remove special tokens and return the translated text\n    return ' '.join(token for token in translation_tokens[1:] if token not in ['<eos>', '<pad>'])\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the trained model\nmodel = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout, device=device)\nmodel.load_state_dict(torch.load('transformer_model.pth'))\nmodel.eval()\nen_vocab = len(en_vocab)\nbn_vocab = len(bn_vocab)\n\n# Enter the English text to translate\nenglish_text = \"Enter your English text here.\"\n\n# Translate the English text to Bengali\nbengali_text = translate_english_to_bengali(model, english_text, en_vocab, bn_vocab, device=device)\n\n# Print the translated text\nprint(\"Translated Bengali text:\", bengali_text)\n","metadata":{},"execution_count":null,"outputs":[]}]}